{"cells":[{"cell_type":"markdown","metadata":{"id":"SW-QXyz1fdc-"},"source":["#CS 4464/7643 Deep Learning HW 3\n","Transformers and Language Modeling\n","In this exercise you will implement a Transformer model and several variants such as Encoder Transformers, Decoder Transformers, and Encoder-Decoder transformers.\n","\n","You will then use these as the basis to train a (small) Language Model from scratch on Google Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ErpQkryF1XC"},"outputs":[],"source":["#@title Colab Setup\n","!pip install datasets\n","!pip install tokenizers\n","!pip install sacrebleu\n","!pip install colab-convert\n","!rm -rf gtGPT/\n","!rm -rf gtgpt\n","!git clone https://github.com/Helw150/gtGPT gtGPT\n","!mv gtGPT/gtgpt/ .\n","\n","from gtgpt.utils import set_seed\n","\n","set_seed(3407)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhpsJ38gF5Pf"},"outputs":[],"source":["import os\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:2\"\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","from gtgpt.model import DummyMultiHeadedSelfAttention, DummyBlock, DummyTransformer, DummyEmbedding\n","from gtgpt.utils import set_seed\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","torch.set_default_device(DEVICE)\n","\n","#Do not change, it will break the AutoGrader\n","setup_block = In[-1]"]},{"cell_type":"markdown","metadata":{"id":"xsNegD_ghJoh"},"source":["#### Embeddings\n","\n","We will first format our input embeddings similarly to how they are constructed in [BERT](https://arxiv.org/pdf/1810.04805.pdf).\n","\n","Recall from lecture that unlike a RNN, a Transformer does not inherently capture positional information in the forward pass. Because of this, we need to add a signal which encodes the position of each token in its embedding.\n","\n","Your first task is to implement the embedding lookup, including the addition of positional encodings. We have already provided the neccesary parameters inside of `DummyEmbedding`.\n","\n","```python\n","self.vocab_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n","self.position_embeddings = nn.Embedding(config.block_size, config.n_embd)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3J4LRVnqF_pT"},"outputs":[],"source":["class Embedding(DummyEmbedding):\n","    def forward(self, idx):\n","        \"\"\"\n","        :param idx: intTensor of shape (B,T)\n","        :returns embeddings: floatTensor of shape (B,T,n_embd)\n","        \"\"\"\n","        B, T = idx.size()\n","        embeddings = None\n","        #############################################################################\n","        # TODO:\n","        # Implement the embedding lookup.                                           #\n","        #                                                                           #\n","        # This will take a few lines.                                               #\n","        #############################################################################\n","\n","        emb = self.vocab_embeddings(idx)\n","\n","        pos_indicies = torch.arange(0, T, device=idx.device).unsqueeze(0).repeat(B, 1)\n","        pos_emb = self.position_embeddings(pos_indicies)\n","        embeddings = emb + pos_emb\n","        #print(embeddings)\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","        return embeddings\n","\n","#Do not change, it will break the AutoGrader\n","embedding_def = In[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFEN1m6jGeDO","cellView":"form"},"outputs":[],"source":["#@title Basic Embedding Test\n","\n","def test_embedding():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_embd = 1\n","  torch.set_default_device(\"cpu\")\n","  set_seed(3047)\n","  embedding = Embedding(config)\n","  embedding.vocab_embeddings.weight = torch.nn.Parameter(torch.arange(0, 10, dtype=torch.float).reshape(10, 1))\n","  embedding.position_embeddings.weight = torch.nn.Parameter(torch.arange(0, 10, dtype=torch.float).reshape(10, 1))\n","  assert torch.allclose(embedding(torch.tensor([[1, 2, 3]])), torch.tensor([1, 3, 5], dtype=torch.float).reshape(1, 3, 1))\n","\n","test_embedding()"]},{"cell_type":"markdown","metadata":{"id":"_N4g9AtKi1Es"},"source":["#### 3.2 Multi-head Self-Attention\n","Attention can be computed in matrix-form using the following formula:\n","\n","$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n","\n","We want to have multiple self-attention operations. Each of these is called a head with each head applied to some portion of the input.\n","\n","$head_i = Attention(W_Q X_i, W_K X_i, W_V X_i)$\n","\n","Here, we'll use GPT-style Multi-headed Self-Attention which fragments the head into pieces and applies one head to each fragment. The fragments are then concatenated together to reconstruct the transformed input and projected with a feed-forward layer.\n","\n","$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$\n","\n","Note that while this is \"Multi-head\", all heads can be computed in parallel with a single matrix multiplication. You can find an in-depth description of this in the reference linked in the code.\n","\n","\n","We provide the needed weights in `DummyMultiHeadedSelfAttention`\n","\n","```python\n","# Note that we need this to be true in GPT-style MHA\n","# Knowing this might come in handy :)\n","assert config.n_embd % config.n_head == 0\n","\n","# Note: These could be a single batched linear layer\n","# but we separate them for simplicity of implementation.\n","self.k = nn.Linear(config.n_embd, config.n_embd)\n","self.v = nn.Linear(config.n_embd, config.n_embd)\n","self.q = nn.Linear(config.n_embd, config.n_embd)\n","# output projection\n","self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","# regularization\n","self.attn_dropout = nn.Dropout(config.attn_pdrop)\n","self.hidden_dropout = nn.Dropout(config.hidden_pdrop)\n","\n","self.n_head = config.n_head\n","self.n_embd = config.n_embd\n","```\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJNOQQgaGip4"},"outputs":[],"source":["class GenericSelfAttention(DummyMultiHeadedSelfAttention):\n","    def forward(self, x, attention_mask):\n","        \"\"\"\n","        :param x: float Tensor of shape (batch size, sequence length, embedding dimensionality)\n","        :param attention_mask: int Tensor of shape (batch size, 1, sequence length, sequence_length)\n","        :returns y: float Tensor of shape (batch size, sequence length, embedding dimensionality)\n","        \"\"\"\n","        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n","        y = None\n","\n","        #############################################################################\n","        # TODO:                                                                     #\n","        # Implement multi-headed self-attention in GPT-2 Style                      #\n","        # Use the provided layers initialized in the DummySelfAttention constructor #\n","        # Apply dropout to the attention values after softmax and the final output  #\n","        #                                                                           #\n","        # Reference:                                                                #\n","        # https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention\n","        #                                                                           #\n","        # Note: All heads should be computed in parallel using the q,k,v layers     #\n","        #                                                                           #\n","        # For each item in the batch, if attention_mask[b, i, j] = 0,               #\n","        # then you should manually set the attention from token i to j to be -inf   #\n","        # Hint: See torch.masked_fill                                               #\n","        #############################################################################\n","        head_dim = C // self.n_head\n","\n","        k=self.k(x).view(B, T, self.n_head, C // self.n_head).transpose(1,2) #key\n","        q=self.q(x).view(B, T, self.n_head, C // self.n_head).transpose(1,2) #querty\n","        v=self.v(x).view(B, T, self.n_head, C // self.n_head).transpose(1,2) #value\n","\n","        att = torch.matmul(q, k.transpose(-2, -1)) / (head_dim ** 0.5)\n","        att = att.masked_fill(attention_mask == 0, float('-inf'))\n","        att = F.softmax(att,dim=-1)\n","        att = self.attn_dropout(att)\n","\n","        y = att @ v #check; uses weighted sum of values using attention weights\n","\n","        y = y.transpose(1,2).contiguous().view(B,T,C) #reshape output\n","\n","        y = self.hidden_dropout(self.c_proj(y))\n","\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","\n","        return y\n","\n","#Do not change, it will break the AutoGrader\n","mha_def = In[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1710200796875,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"},"user_tz":240},"id":"TOONbP93GrGT","outputId":"e0bf591c-cfbf-42c5-90ef-c545e115065a","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Success 1\n","Success 2\n"]}],"source":["#@title Test Multi-Headed Attention\n","\n","def test_mha():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_embd = 4\n","  config.n_head = 2\n","  config.hidden_pdrop = 0.25\n","  config.attn_pdrop = 0.1\n","  set_seed(3407)\n","  torch.set_default_device(\"cpu\")\n","  attn = GenericSelfAttention(config)\n","  attn.q.weight = torch.nn.Parameter(torch.eye(2, 2).repeat(2, 2).flip(0))\n","  attn.q.bias = torch.nn.Parameter(torch.zeros(4))\n","  attn.k.weight = torch.nn.Parameter(torch.eye(2, 2).repeat(2, 2))\n","  attn.k.bias = torch.nn.Parameter(torch.zeros(4))\n","  attn.v.weight = torch.nn.Parameter(torch.eye(4, 4))\n","  attn.v.bias = torch.nn.Parameter(torch.zeros(4))\n","  attn.c_proj.weight = torch.nn.Parameter(torch.eye(4, 4))\n","  attn.c_proj.bias = torch.nn.Parameter(torch.zeros(4))\n","  embeddings = torch.tensor([[[1, 2, 3, 4] ,[4, 3, 2, 1]]], dtype=torch.float)\n","  mask = torch.ones(1, 2, 2)\n","  assert torch.allclose(attn(embeddings, mask), torch.tensor([[[5.6779, 0, 0, 0], [0, 3.0456, 4.3618, 5.6779]]], dtype=torch.float), atol=1e-3, rtol=1)\n","  print(\"Success 1\")\n","\n","test_mha()\n","\n","def test_mha_mask():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_embd = 4\n","  config.n_head = 2\n","  config.hidden_pdrop = 0.0\n","  config.attn_pdrop = 0.0\n","  set_seed(3407)\n","  torch.set_default_device(\"cpu\")\n","  attn = GenericSelfAttention(config)\n","  attn.v.weight = torch.nn.Parameter(torch.eye(4, 4))\n","  attn.v.bias = torch.nn.Parameter(torch.zeros(4))\n","  attn.c_proj.weight = torch.nn.Parameter(torch.eye(4, 4))\n","  attn.c_proj.bias = torch.nn.Parameter(torch.zeros(4))\n","  embeddings = torch.tensor([[[1, 2, 3, 4] ,[4, 3, 2, 1]]], dtype=torch.float)\n","  mask = torch.zeros(1, 2, 2)\n","  mask[0, 0, 0] = 1\n","  mask[0, 1, 1] = 1\n","  assert torch.allclose(attn(embeddings, mask), torch.tensor([[[1, 2, 3, 4], [4, 3, 2, 1]]], dtype=torch.float), atol=1e-4)\n","  print(\"Success 2\")\n","\n","test_mha_mask()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-LkiiMDG0iN","cellView":"form"},"outputs":[],"source":["#@title Now, we can very simply create a single layer transformer block!\n","class TransformerBlock(DummyBlock):\n","    def __init__(self, config):\n","        super().__init__(config, GenericSelfAttention)\n","\n","    # A Basic Transformer Block with Attention followed by an MLP\n","    # note the layer norms and residual information preserved at each step.\n","    def forward(self, x, attention_mask):\n","        x = x + self.attn(self.ln_1(x), attention_mask)\n","        x = x + self.mlpf(self.ln_2(x))\n","        return x\n","\n","#Do not change, it will break the AutoGrader\n","block_def = In[-1]"]},{"cell_type":"markdown","metadata":{"id":"0XlolMIhnfBa"},"source":["#### Putting it all together\n","\n","Using our Embedding Layer, the above Transformer Block using our Multi-head attention, and a simple classification head we have all the pieces we need for a Transformer language model.\n","\n","For the forward pass, you'll want to first embed our inputs, apply each transformer layer sequentially, and finally get logits for each possible output word using a classification layer (often called a language modeling head).\n","\n","If an argument is passed to `hidden_cache`, you should prepend it to your input embeddings and pass it alongside the embeddings for the rest of the model. This will allow use to use this structure in Encoder-Decoder architectures later, but also allows passing vectors from any other neural network (such as a computer vision model or an audio model to enable multi-modal understanding). You can find a rich description of how these pieces come together [here](https://jalammar.github.io/illustrated-transformer/).\n","\n","All the parameters you'll need come from `DummyTransformer` and the code blocks above your code section.\n","\n","```python\n","self.transformer = nn.ModuleDict(\n","    dict(\n","        embedding=embedding(config),\n","        h=nn.ModuleList(\n","            [block(config) for _ in range(config.n_layer)]\n","        ),\n","        ln_f=nn.LayerNorm(config.n_embd),\n","    )\n",")\n","self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJCBDuLzncKd"},"outputs":[],"source":["class GenericTransformer(DummyTransformer):\n","    def __init__(self, config):\n","        super().__init__(config, TransformerBlock, Embedding)\n","        self.block_size = config.block_size # Maximum Number of Tokens which can be encoded at once\n","        self.vocab_size = config.vocab_size\n","\n","    def get_attention_mask(self, num_tokens):\n","        \"\"\"\n","        Dummy For now, we will see how we use this later!\n","        \"\"\"\n","        B = num_tokens.shape[0]\n","        return torch.ones((B, self.block_size, self.block_size))[:, :num_tokens.max().item(), :num_tokens.max().item()]\n","\n","    def forward(self, idx, targets=None, hidden_cache=None, return_hidden=False):\n","        \"\"\"\n","        :param idx: int Tensor of shape (B,T)\n","        :param hidden_cache: float Tensor of shape (B,P_T,n_embd)\n","        :param targets: int Tensor of shape (B,T_T)\n","        :param return_hidden: bool\n","        (if return_hidden = None)\n","        :returns x: float Tensor of shape (B,T,n_embd)\n","        (else)\n","        :returns logits: float Tensor of shape (B, T, vocab_size)\n","        :returns loss: float Tensor of shape (B) or None\n","        \"\"\"\n","        num_tokens = (idx != -1).type(torch.int).sum(dim=1)\n","        if hidden_cache is not None:\n","          num_tokens = num_tokens + hidden_cache.shape[1]\n","        idx = idx.masked_fill(idx == -1, int(0)).type(torch.int)[:, :num_tokens.max().item()]\n","        if targets is not None:\n","          targets = targets[:, :num_tokens.max().item()]\n","        attention_mask = self.get_attention_mask(num_tokens)\n","        #############################################################################\n","        # TODO:                                                                     #\n","        # Put all the modules of a Transformer together for inference               #\n","        #                                                                           #\n","        # If hidden_cache exists,                                                   #\n","        # then the Transformer inputs should be concatenated in the token dimension #\n","        # First) All Embeddings from Hidden Cache                                   #\n","        # Next)  All Embeddings of tokens from idx.                                 #\n","        #                                                                           #\n","        # All the modules you'll need are listed here:                              #\n","        #                                                                           #\n","        # Note: You can iterate through a nn.ModuleList using a standard for loop.  #\n","        #                                                                           #\n","        # This will take a few lines!                                               #\n","        ##############################################################################\n","        if hidden_cache is not None:\n","          #checks if embedding has zero shape\n","          if self.transformer['embedding'](idx).shape[1] == 0:\n","            x = hidden_cache\n","          else:\n","            x = torch.cat([hidden_cache, self.transformer['embedding'](idx)], dim = 1) #otherwise, concat along dim 1\n","\n","          for module in self.transformer['h']:\n","            x = module(x, attention_mask)\n","            x = self.transformer['ln_f'](x)\n","          logits = self.lm_head(x) #obtain logits from language model head\n","          #print(logits)\n","        else:\n","          x = self.transformer['embedding'](idx)\n","          for module in self.transformer['h']:\n","            x = module(x, attention_mask)\n","            x = self.transformer['ln_f'](x)\n","          logits = self.lm_head(x) #same as above\n","          #print(logits)\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","        if return_hidden:\n","            return x\n","\n","        # if we are given some desired targets also calculate the loss\n","        loss = None\n","        if targets is not None:\n","            s_logits = logits\n","            if hidden_cache is not None:\n","              s_logits = logits[:, hidden_cache.shape[1]-1:-1].contiguous()\n","              #print(logits[-1].argmax(dim=1))\n","            loss = F.cross_entropy(\n","                s_logits.reshape(-1, self.vocab_size), targets.reshape(-1), ignore_index=-1\n","            )\n","\n","\n","        return logits, loss\n","\n","#Do not change, it will break the AutoGrader\n","transformer_def = In[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1710200797332,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"},"user_tz":240},"id":"GvahgUP5G04t","outputId":"bcc67445-b915-4849-c2b0-330fade57ead","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.00M\n","Success 1\n","number of parameters: 0.00M\n","Success 2\n","number of parameters: 0.00M\n","Success 3\n"]}],"source":["#@title Test Full Transformer Forward Pass\n","\n","def test_transformer():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_layer = 2\n","  config.n_embd = 4\n","  config.n_head = 2\n","  torch.set_default_device(\"cpu\")\n","  set_seed(3407)\n","  transformer = GenericTransformer(config)\n","  idx = torch.tensor([[1, 2, 3, 4, 5, -1, -1, -1, -1, -1]], dtype=torch.long)\n","  s = F.softmax(transformer(idx)[0][0, 0], dim=0)\n","  assert torch.allclose(s, torch.tensor([0.1034, 0.0960, 0.1019, 0.1022, 0.1003, 0.1040, 0.0983, 0.1072, 0.0958, 0.0910], dtype=torch.float), atol=1e-5, rtol=1)\n","  print(\"Success 1\")\n","\n","def test_transformer_loss():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_layer = 2\n","  config.n_embd = 4\n","  config.n_head = 2\n","  torch.set_default_device(\"cpu\")\n","  set_seed(3407)\n","  transformer = GenericTransformer(config)\n","  idx = torch.tensor([[1, 2, 3, 4, 5, -1, -1, -1, -1, -1]], dtype=torch.long)\n","  target = torch.arange(5).reshape(1, 5)\n","  assert torch.allclose(transformer(idx, targets=target)[1], torch.tensor(2.2973))\n","  print(\"Success 2\")\n","\n","def test_transformer_hidden():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_layer = 2\n","  config.n_embd = 4\n","  config.n_head = 2\n","  torch.set_default_device(\"cpu\")\n","  set_seed(3407)\n","  transformer = GenericTransformer(config)\n","  idx = torch.tensor([[1, 2, 3, 4, 5, -1, -1, -1, -1, -1]], dtype=torch.long)\n","  target = torch.arange(5).reshape(1, 5)\n","  hidden = transformer(idx, targets=target, return_hidden=True)\n","  assert torch.allclose(hidden[0, 0], torch.tensor([1.4417, -1.3564,  0.1549, -0.2401]), atol=1e-4)\n","  print(\"Success 3\")\n","\n","test_transformer()\n","test_transformer_loss()\n","test_transformer_hidden()"]},{"cell_type":"markdown","metadata":{"id":"TYNuYWMn0iaS"},"source":["#### Implement an Encoder Transformer\n","\n","Encoders, like the BERT model you learned about in lecture, utilize bi-directional attention. This means that in the sequence \"A B C\", the representation for token \"B\" will be influenced by tokens A *and* C. When all tokens can attend to all other tokens, the attention_mask is just a matrix of ones.\n","\n","However, since sentences come in a wide range of lengths, we need a way to batch sequences of different lengths together in order to maximize our GPU throughput. The most common way of doing this is called \"Padding\". When you pad an input, you add additional \"pad\" tokens to make it the same length as the longest sequence in a batch. For example, if we wanted to batch \"A B C\" and \"A B C D\" together, we would add a \"pad\" token to \"A B C\". Our resulting batch would be \\[\"A B C \\<pad\\>\", \"A B C D\"\\].\n","\n","Since these pad tokens are meaningless, we want to avoid having them affect our results. To do this, we remove them from the attention mask for that element in the batch. Below, you'll write a function to create such an attention mask for padded sequences given a tensor which contains the number of valid leading tokens for each batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VF8grwOtG-gs"},"outputs":[],"source":["class Encoder(GenericTransformer):\n","    \"\"\"Encoder Style Transformer with Bidirectional Attention\"\"\"\n","    def get_attention_mask(self, num_tokens):\n","        \"\"\"\n","        :param num_tokens: int Tensor of shape (batch size)\n","        :returns attention_mask: int tensor of shape (batch_size, 1, max_tokens, max_tokens)\n","        \"\"\"\n","        B = num_tokens.shape[0]\n","        max_tokens = min(self.block_size, num_tokens.max().item())\n","        ##############################################################################\n","        # TODO:                                                                      #\n","        # Implement a padding mask function.                                         #\n","        # This allows batching sequences of different lengths.                       #\n","        #                                                                            #\n","        # For example, for any row attention_mask[b, i] the following should be true:#\n","        #               For j < num_tokens[b], attention_mask[b, i, j] = 1          #\n","        #               For j >= num_tokens[b],  attention_mask[b, i, j] = 0         #\n","        #                                                                            #\n","        # Reference:https://huggingface.co/docs/transformers/glossary#attention-mask #                                                                #\n","        #                                                                            #\n","        # This should be a 1-3 line function.                                        #\n","        ##############################################################################\n","        attention_mask = torch.arange(max_tokens).unsqueeze(0)\n","\n","        attention_mask = torch.cat([attention_mask]*max_tokens,dim=0)\n","\n","        num_tokens=num_tokens.reshape(B,1,1)\n","\n","        attention_mask = torch.where(attention_mask < num_tokens, 1, 0)\n","        #print(attention_mask.numpy())\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","        return attention_mask.reshape(B, 1, max_tokens, max_tokens)\n","\n","#Do not change, it will break the AutoGrader\n","encoder_def = In[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1710200797332,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"},"user_tz":240},"id":"pXGGTqi4G-6i","outputId":"c2dc6a83-86b5-49c7-9ffb-95da2e4f4797","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.00M\n"]}],"source":["#@title Test Encoder\n","\n","def test_encoder():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_layer = 2\n","  config.n_embd = 4\n","  config.n_head = 2\n","  torch.set_default_device(\"cpu\")\n","  set_seed(3407)\n","  transformer = Encoder(config)\n","  mask = transformer.get_attention_mask(torch.tensor([5, 6]))\n","  assert mask[0, :, 0].sum() == 5\n","  assert mask[1, :, 0].sum() == 6\n","\n","\n","test_encoder()"]},{"cell_type":"markdown","metadata":{"id":"wfA1ktrT2l0a"},"source":["#### Implement an Decoder Transformer\n","\n","Unlike Encoders, Decoders are a \"causal\" model, meaning that each prediction is only influenced by the tokens which came earlier than it in the input. While \"Encoders\" and \"Decoders\" are often discussed as different types of models, the only core difference is the attention mask used.\n","\n","For decoders, we want the attention mask for each token to only include the previous tokens in the sequence. Despite being functionally very different models, a \"Decoder\" can be implemented with just a one line change of the \"Encoder\" attention mask. You'll implement this below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"je6nfibPHGPr"},"outputs":[],"source":["class Decoder(Encoder):\n","    \"\"\"Decoder Style model with a Causal Attention Mask\"\"\"\n","\n","    def get_attention_mask(self, num_tokens):\n","        \"\"\"\n","        :param num_tokens: int Tensor of shape (batch size)\n","        :returns attention_mask: int tensor of shape (batch_size, 1, block_size, block_size)\n","        \"\"\"\n","        full_attention_mask = super().get_attention_mask(num_tokens)\n","        ##############################################################################\n","        # TODO:                                                                      #\n","        # Modify the output of the full encoder mask to create a \"causal\" mask       #\n","        # such that tokens only attend to tokens which occured earlier in the input. #\n","        #                                                                            #\n","        # For example, for any row attention_mask[b, i} the following should be true:#\n","        #               For j <= i, attention_mask[b, i, j] = 1                      #\n","        #               For j > i,  attention_mask[b, i, j] = 0                      #\n","        #                                                                            #\n","        # This should be a one line function which modifies the full attention_mask  #\n","        ##############################################################################\n","        attention_mask =torch.tril(full_attention_mask, diagonal=0)\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","        return attention_mask\n","\n","#Do not change, it will break the AutoGraderDD\n","decoder_def = In[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1710200797332,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"},"user_tz":240},"id":"nyg5zzmc_8y0","outputId":"1d9bb7af-836e-4d35-fca0-aaccb4509392","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.00M\n"]}],"source":["#@title Test Decoder\n","\n","def test_decoder():\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = 10\n","  config.block_size = 10\n","  config.n_layer = 2\n","  config.n_embd = 4\n","  config.n_head = 2\n","  torch.set_default_device(\"cpu\")\n","  set_seed(3407)\n","  transformer = Decoder(config)\n","  mask = transformer.get_attention_mask(torch.tensor([[5], [6]]))\n","  for i in range(5):\n","    assert mask[0, :, i].sum() == i+1\n","  assert mask[0, :, 5].sum() == 5\n","  for i in range(6):\n","    assert mask[1, :, i].sum() == i+1\n","\n","test_decoder()"]},{"cell_type":"markdown","metadata":{"id":"UABwUKI93c_A"},"source":["#### Implement an Decoder Transformer\n","\n","Unlike Encoders, Decoders are a \"causal\" model, meaning that each prediction is only influenced by the tokens which came earlier than it in the input. While \"Encoders\" and \"Decoders\" are often discussed as different types of models, the only core difference is the attention mask used.\n","\n","For decoders, we want the attention mask for each token to only include the previous tokens in the sequence. Despite being functionally very different models, a \"Decoder\" can be implemented with just a one line change of the \"Encoder\" attention mask. You'll implement this below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEbiacH3JUF8"},"outputs":[],"source":["def generate(model, idx, max_new_tokens, temperature=1.0):\n","    \"\"\"\n","    :param idx: int Tensor of shape (B, T)\n","    :param max_new_tokens: int\n","    :param temperature: Float\n","    :returns idx: int Tensor of shape (B, T+max_new_tokens)\n","    \"\"\"\n","    ##############################################################################\n","    # TODO:                                                                      #\n","    # Sample from your model max_new_tokens times                                #\n","    # You should feed the predictions back into the model each time              #\n","    #                                                                            #\n","    # Adjust the probability distribution to be more or less greedy using        #\n","    # the temperature parameter                                                  #\n","    #                                                                            #\n","    # Reference: https://huggingface.co/blog/how-to-generate#sampling            #\n","    # Temperature Reference:                                                     #\n","    # https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture10-nlg.pdf#page=34 #\n","    ##############################################################################\n","\n","    b,t = idx.size()\n","    m = nn.Softmax(dim=-1)\n","\n","    for i in range(max_new_tokens):\n","      a =m(model(idx)[0][:,-1,:]/temperature)\n","      j = torch.distributions.Categorical(probs=a) #change based on probs of softmax\n","      samples = j.sample()\n","      samples = samples.unsqueeze(0) #to match to idx's shape\n","      idx = torch.cat((idx, samples), dim = -1)\n","      #print(\"tok\", i+1, \":\", samples.item())\n","    ##############################################################################\n","    #                               END OF YOUR CODE                             #\n","    ##############################################################################\n","    return idx\n","\n","#Do not change, it will break the AutoGrader\n","generate_def = In[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UR1Zn12LanRo"},"outputs":[],"source":["#@title Test Generation\n","\n","def test_generate():\n","    def dumb_model(idx):\n","      l = torch.zeros(1, 1, 10)\n","      l[0, 0, 0] = 100\n","      l[0, 0, 5] = 90\n","      return l.roll(idx[0, -1].item()+1), None\n","    torch.set_default_device(\"cpu\")\n","    set_seed(3047)\n","    assert torch.allclose(generate(dumb_model, torch.tensor([[0]]), 6), torch.tensor([0,1,2,3,4,5,6]))\n","    temp_gen_1 = generate(dumb_model, torch.tensor([[0]]), 6, temperature=10)\n","    assert torch.allclose(temp_gen_1, torch.tensor([[0, 6, 2, 3, 4, 5, 6]]))\n","\n","test_generate()"]},{"cell_type":"markdown","metadata":{"id":"kYnbvnKI3gnF"},"source":["#### Implement an Encoder Decoder Transformer\n","\n","Now, we'll put together our Encoder and Decoder models. This combination of the two architectures allows us to maximize the signal we can draw from the input using a bi-directional encoder, while generating language using a causal decoder.\n","\n","Below, you'll combine your Encoder and Decoder classes in a forward function making use of the `return_hidden` and `hidden_cache` arguments we supported in our Transformer implementation to pass information between the modules."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5L7c5YifKd-"},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    \"\"\"Encoder-Decoder Model which combines the two architectures\"\"\"\n","    def __init__(self, encoder_config, decoder_config):\n","        super().__init__()\n","        # Add end of sequence token.\n","        decoder_config.vocab_size += 1\n","        self.vocab_size = decoder_config.vocab_size\n","        self.encoder = Encoder(encoder_config)\n","        self.decoder = Decoder(decoder_config)\n","\n","    def configure_optimizers(self, train_config):\n","        enc_groups = self.encoder.configure_optimizers(train_config)\n","        dec_groups = self.decoder.configure_optimizers(train_config)\n","        return enc_groups + dec_groups\n","\n","    def forward(self, prefix, targets=None):\n","        \"\"\"\n","        :param prefix: int Tensor of shape (B,P_T)\n","        :param idx: float Tensor of shape (B,P_T,n_embd)\n","        :returns logits: float Tensor of shape (B, vocab_size)\n","        :returns loss: float Tensor of shape (B) or None\n","        \"\"\"\n","        B = prefix.shape[0]\n","        idx = torch.tensor([[]]).repeat(B, 1)\n","        if targets is not None:\n","          idx = torch.cat((idx, targets), dim=1)\n","\n","        ##############################################################################\n","        # TODO:                                                                      #\n","        # Create an Encoder Decoder Model by combining your previous transformers    #\n","        # The Encoder should encode the tokens from prefix into an embeddings        #\n","        # Use these in the hidden_cache to condition decoder generation              #\n","        #                                                                            #\n","        # This should be a 1-2 lines.                                                #\n","        ##############################################################################\n","        embed = self.encoder(prefix, return_hidden = True)\n","        logits, loss = self.decoder(idx, targets = targets, hidden_cache = embed, return_hidden = False)\n","        #print(\"log:\", logits)\n","        #print(\"loss:\", loss)\n","        ##############################################################################\n","        #                               END OF YOUR CODE                             #\n","        ##############################################################################\n","        return logits, loss\n","\n","#Do not change, it will break the AutoGrader\n","encdec_def = In[-1]"]},{"cell_type":"markdown","metadata":{"id":"wwTVJHeG4tVx"},"source":["This will also require a custom `prefix_generation` function to account for the distinction between a human provided `prefix` and a model generated `idx` in the Encoder Decoder forward pass.\n","\n","Don't worry, this should be only a small change from your original `generate` function above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0X8dl3D64uql"},"outputs":[],"source":["def prefix_generate(model, prefix, max_new_tokens, temperature=1.0):\n","    \"\"\"\n","    :param prefix: int Tensor of shape (B, T)\n","    :param max_new_tokens: int\n","    :param temperature: Float\n","    :returns idx: int Tensor of shape (B, max_new_tokens)\n","    \"\"\"\n","    idx = torch.tensor([[]], dtype=torch.long)\n","    ##############################################################################\n","    # TODO:                                                                      #\n","    # Adjust your original generation function to work Encoder-Decoder models    #\n","    #                                                                            #\n","    # Note: This should be a one line change from the original generate function #\n","    ##############################################################################\n","\n","    #deleted line. everything else is same\n","    m = nn.Softmax(dim=-1)\n","\n","    for i in range(max_new_tokens):\n","      a = m(model(prefix, targets = idx)[0][:,-1,:]/ temperature)\n","      j = torch.distributions.categorical(probs=a)\n","      samples = j.sample()\n","      samples = samples.unsqueeze(0)\n","      idx = torch.cat((idx, samples), dim = -1)\n","    ##############################################################################\n","    #                               END OF YOUR CODE                             #\n","    ##############################################################################\n","    return idx\n","\n","#Do not change, it will break the AutoGrader\n","pref_generate_def = In[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kICaKJURf9-L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710200824638,"user_tz":240,"elapsed":27312,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"}},"outputId":"9e524064-aaf7-4237-8f5e-90c5ff1b1093"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 0.09M\n","number of parameters: 0.09M\n","running on device cpu\n","iter_dt 0.00ms; iter 0: train loss 1.44665\n","iter_dt 52.81ms; iter 100: train loss 0.12477\n","iter_dt 67.45ms; iter 200: train loss 0.03320\n","iter_dt 51.55ms; iter 300: train loss 0.00456\n","iter_dt 71.31ms; iter 400: train loss 0.03842\n"]}],"source":["#@title End to End Test of Encoder Decoder Training\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","from gtgpt.trainer import Trainer\n","\n","import pickle\n","\n","class SortDataset(Dataset):\n","    \"\"\"\n","    Dataset for the Sort problem. E.g. for problem length 6:\n","    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n","    Which will feed into the transformer concatenated as:\n","    input:  0 0 2 1 0 1 0 0 0 1 1\n","    output: I I I I I 0 0 0 1 1 2\n","    where I is \"ignore\", as the transformer is reading the input sequence\n","    \"\"\"\n","\n","    def __init__(self, split, length=6, num_digits=3):\n","        assert split in {'train', 'test'}\n","        self.split = split\n","        self.length = length\n","        self.num_digits = num_digits\n","\n","    def __len__(self):\n","        return 10000 # ...\n","\n","    def get_vocab_size(self):\n","        return self.num_digits\n","\n","    def get_block_size(self):\n","        # the length of the sequence that will feed into transformer,\n","        # containing concatenated input and the output, but -1 because\n","        # the transformer starts making predictions at the last input element\n","        return 20\n","\n","    def __getitem__(self, idx):\n","\n","        # use rejection sampling to generate an input example from the desired split\n","        while True:\n","            # generate some random integers\n","            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n","            # half of the time let's try to boost the number of examples that\n","            # have a large number of repeats, as this is what the model seems to struggle\n","            # with later in training, and they are kind of rate\n","            if torch.rand(1).item() < 0.5:\n","                if inp.unique().nelement() > self.length // 2:\n","                    # too many unqiue digits, re-sample\n","                    continue\n","            # figure out if this generated example is train or test based on its hash\n","            h = hash(pickle.dumps(inp.tolist()))\n","            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n","            if inp_split == self.split:\n","                break # ok\n","\n","        # solve the task: i.e. sort\n","        sol = torch.sort(inp)[0]\n","\n","        # concatenate the problem specification and the solution\n","        cat = torch.cat((inp, sol), dim=0)\n","\n","        # the inputs to the transformer will be the offset sequence\n","        x = cat[:self.length].clone()\n","        y = cat[self.length:].clone()\n","        # we only want to predict at output locations, mask out the loss at the input locations\n","        return x, y\n","\n","def test_encoder_decoder():\n","  # print an example instance of the dataset\n","  train_dataset = SortDataset('train')\n","  test_dataset = SortDataset('test')\n","  x, y = train_dataset[0]\n","  config = DummyTransformer.get_default_config()\n","  config.vocab_size = train_dataset.get_vocab_size()\n","  config.block_size = train_dataset.get_block_size()\n","  config.n_layer = 3\n","  config.n_embd = 48\n","  config.n_head = 3\n","  torch.set_default_device(\"cpu\")\n","  set_seed(3407)\n","  model = EncoderDecoder(config, config)\n","  train_config = Trainer.get_default_config()\n","  train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n","  train_config.max_iters = 500\n","  train_config.num_workers = 0\n","  train_config.device = \"cpu\"\n","  trainer = Trainer(train_config, model, train_dataset)\n","  def batch_end_callback(trainer):\n","      if trainer.iter_num % 100 == 0:\n","          print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n","  trainer.set_callback('on_batch_end', batch_end_callback)\n","\n","  trainer.run()\n","  model.eval()\n","  assert torch.allclose(prefix_generate(model, torch.tensor([[2, 1, 1, 0, 1, 2]]), max_new_tokens=6), torch.tensor([[0, 1, 1, 1, 2, 2]]))\n","\n","test_encoder_decoder()"]},{"cell_type":"markdown","metadata":{"id":"I3xuBrm4449O"},"source":["# You've implemented a language model!\n","\n","1.   List item\n","2.   List item\n","\n","\n","\n","## Now let's put it to use\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9XRRfet5CIL"},"outputs":[],"source":["#@title Language Modeling Setup (Do Not Change)\n","from gtgpt.trainer import Trainer\n","from tqdm import tqdm\n","from tokenizers import Tokenizer\n","from tokenizers.pre_tokenizers import ByteLevel\n","from tokenizers.trainers import UnigramTrainer, BpeTrainer\n","from tokenizers.models import Unigram, BPE\n","from datasets import load_dataset\n","import random\n","\n","class LMDataset(Dataset):\n","    def __init__(self, split, data, tokenizer, model):\n","        assert split in {'train', 'test'}\n","        self.model_type = \"EncDec\" if issubclass(type(model), EncoderDecoder) else \"Dec\"\n","        if split == \"train\":\n","          self.start_split = 0\n","          self.end_split = 30000\n","        else:\n","          self.start_split = 30000\n","          self.end_split = 40000\n","        self.split = split\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.block_size = max([len(self.tokenizer.encode(inp)) for inp in self.data])\n","        self.process()\n","\n","    def __len__(self):\n","        return len(self.data[self.start_split:self.end_split])\n","\n","    def get_vocab_size(self):\n","        return self.tokenizer.get_vocab_size()\n","\n","    def get_block_size(self):\n","        # the length of the sequence that will feed into transformer,\n","        # containing concatenated input and the output, but -1 because\n","        # the transformer starts making predictions at the last input element\n","        return self.block_size\n","\n","    def process(self):\n","      new_data = []\n","      for inp in tqdm(self.data):\n","        if self.model_type == \"EncDec\":\n","          x_inp = inp.split(\"[SEP]\")[0] + \"[SEP]\"\n","          y_inp = inp.split(\"[SEP]\")[1]\n","          x = self.tokenizer.encode(x_inp)\n","          y = self.tokenizer.encode(y_inp)\n","        else:\n","          x = self.tokenizer.encode(inp)\n","          y = x[1:]\n","          x = x[:-1]\n","        x = x + ([-1] * (self.get_block_size() - len(x)))\n","        y = y + ([-1] * (self.get_block_size() - len(y)))\n","        new_data.append((x, y))\n","      self.data = new_data\n","\n","    def __getitem__(self, idx):\n","      x, y = self.data[self.start_split + idx]\n","      return torch.tensor(x), torch.tensor(y)\n","\n","def format_review(row):\n","  return {\"text\": f\"{row['translation']['eng']}[SEP]{row['translation']['engyay']}[END]\"}\n","\n","dataset = load_dataset(\"cdleong/piglatin-mt\")[\"train\"]\n","data = [row[\"text\"] for row in dataset.map(format_review).to_list()]\n","set_seed(3047)\n","random.shuffle(data)"]},{"cell_type":"markdown","metadata":{"id":"UcvP7ZXLEoq0"},"source":["#### Training a Language Model from Scratch\n","\n","Above, we set up code which loads WikiHow articles as a training dataset either for pure Decoder models or with the Title passed as a prefix for an EncoderDecoder model. We want to train a model to translate between English and Pig-Latin!\n","\n","Below is an implementation which achieves between 40 and 50 percent accuracy. Modify the tokenizer, architecture, or hyperparameters to  decrease the loss as much as possible and drive accuracy above 80%. Report what you changed and your intuitions for why you changed it in the report powerpoint file and upload it as a PDF to GradeScope.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBbSkVLq5Eir"},"outputs":[],"source":["# Incredibly Simplified Tokenizer so that you can manually hack it!\n","# Feel free to add special tokens or modify as you wish.\n","# For real world tokenizer usage, see https://huggingface.co/docs/tokenizers/\n","class Tokenizer():\n","  def __init__(self):\n","    self.DELIM = \"|[DELIM]|\"\n","    self.special_tokens = [\"[SEP]\", \"[END]\"]\n","    self.special_tokens = [self.stringify(list(bytes(tok, \"utf-8\"))) for tok in self.special_tokens]\n","    self.vocab_size = 256 + len(self.special_tokens)\n","\n","  def stringify(self, b_enc):\n","    s_enc = [str(b) for b in b_enc]\n","    return self.DELIM.join(s_enc)\n","\n","  def get_vocab_size(self):\n","    return self.vocab_size\n","\n","  def encode(self, inp):\n","    s_enc = self.stringify(list(bytes(inp, \"utf-8\")))\n","    for i, tok in enumerate(self.special_tokens):\n","      s_enc = s_enc.replace(tok, str(255+i+1))\n","    return [int(s) for s in s_enc.split(self.DELIM)]\n","\n","  def decode(self, inp):\n","    s_enc = self.stringify(inp)\n","    for i, tok in enumerate(self.special_tokens):\n","      s_enc = s_enc.replace(str(255+i+1), tok)\n","    return  bytes([int(c) for c in s_enc.split(self.DELIM)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cfrrAkfDIQT"},"outputs":[],"source":["from tqdm import tqdm\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","torch.set_default_device(DEVICE)\n","def train(data, model_type=\"Decoder\",\n","          learning_rate = 5e-4,\n","          batch_size = 32,\n","          max_iters = 10000,\n","          dec_n_layer=6,\n","          dec_n_embd=128,\n","          dec_n_head = 4,\n","          enc_n_layer=6,\n","          enc_n_embd=128,\n","          enc_n_head=4):\n","  # Model Setup\n","  tokenizer = Tokenizer()\n","  dec_config = DummyTransformer.get_default_config()\n","  dec_config.vocab_size = tokenizer.get_vocab_size()\n","  dec_config.block_size = max([len(tokenizer.encode(inp)) for inp in data])\n","  dec_config.n_layer = dec_n_layer\n","  dec_config.n_embd = dec_n_embd\n","  dec_config.n_head = dec_n_head\n","  if model_type == \"Decoder\":\n","    model = Decoder(dec_config)\n","  else:\n","    enc_config = DummyTransformer.get_default_config()\n","    enc_config.vocab_size = tokenizer.get_vocab_size()\n","    enc_config.block_size = max([len(tokenizer.encode(inp)) for inp in data])\n","    enc_config.n_layer = enc_n_layer\n","    enc_config.n_embd = enc_n_embd\n","    enc_config.n_head = enc_n_head\n","    model = EncoderDecoder(enc_config, dec_config)\n","\n","  # Training Config\n","  train_config = Trainer.get_default_config()\n","  train_config.learning_rate = learning_rate\n","  train_config.max_iters = max_iters\n","  train_config.batch_size = batch_size\n","  train_config.num_workers = 0\n","  train_config.device = DEVICE\n","  train_ds = LMDataset(\"train\", data, tokenizer, model)\n","  # Training Loop\n","  trainer = Trainer(train_config, model, train_ds)\n","  def batch_end_callback(trainer):\n","      if trainer.iter_num % 100 == 0:\n","          tqdm.write(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n","          prefix = torch.tensor([tokenizer.encode(\"translate this to piglatin[SEP]\")])\n","          if model_type == \"Decoder\":\n","            output = generate(model, prefix, 100, 0.1)\n","          else:\n","            output = prefix_generate(model, prefix, 100, 0.1)\n","          print(tokenizer.decode(output.cpu().numpy()[0]).split(bytes(\"[END]\", \"utf-8\"))[0])\n","  trainer.set_callback('on_batch_end', batch_end_callback)\n","  trainer.run()\n","  return model, trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7WzG16__KFU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710208280192,"user_tz":240,"elapsed":753106,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"}},"outputId":"16923287-c5f8-42b9-ee73-fcba10c357ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 1.25M\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13232/13232 [00:01<00:00, 8280.76it/s] \n"]},{"output_type":"stream","name":"stdout","text":["running on device cuda:0\n","iter_dt 0.00ms; iter 0: train loss 5.56252\n","b'translate this to piglatin[SEP]\\xb3\\xc8\\x1d\\xc6$C\\x88M\\xd2 e\\xc1q\\xc1\\xb5h\\x87\\xf1ke\\xaf\\xc8\\xe9\\xe1cni\\xbe o\\xd2\\x9d\\x08e%\\x9bv\\xd8m6 9\\x82\\xe7\\xd3\\xbfc(\\xc8k1\\xd1\\x98\\xfd\\x1d\\x07\\xf3e\\xcc\\xd9/~\\x94J\\xb1,e\\xa4\\xd2`\\x860s%L \\xd2\\x84\\xf3DrH\\xb1>\\xeem\\xb7~O\\xa8c\\xddm\\xee!\\x8b[8e['\n","iter_dt 57.22ms; iter 100: train loss 2.10138\n","b'translate this to piglatin[SEP]ay orerererer ond oreren thererere and-thay ond-ay ay on-thay e-thay e-ay e-ay e-t-ay e-ay oray e-t-'\n","iter_dt 54.09ms; iter 200: train loss 2.05093\n","b'translate this to piglatin[SEP]an-ay on-tay ay onghan-thay an-ay and-thay e-ay ay e-tay an-thay e-thay e-thay o-thay e-thay an-thay'\n","iter_dt 58.63ms; iter 300: train loss 1.89816\n","b'translate this to piglatin[SEP]e-thay or-tay or-thay or-thay in-thay er-thay eray e-th-thay ont-thay e-thay e-thay e-thay o-thay on'\n","iter_dt 54.64ms; iter 400: train loss 1.85297\n","b'translate this to piglatin[SEP]e-thay or-thay e-thay or-thay or-thay orere-t[SEP]athay e-th-thay an-thay e-thay an-thay e-thay at-thay '\n","iter_dt 52.66ms; iter 500: train loss 1.76936\n","b'translate this to piglatin[SEP]ent-thay on-thay e-thay of-thay ond-they ther[SEP]e-thay or-thay e-thay e-thay eithe-thay at-thay of-tha'\n","iter_dt 53.93ms; iter 600: train loss 1.73647\n","b'translate this to piglatin[SEP]eritir-tay ont-ay e-thay ore-tay artered the[SEP]e-thay and-ay of-ay e-thay e-thay end-ay ar-ay of-ay o-'\n","iter_dt 55.35ms; iter 700: train loss 1.68990\n","b'translate this to piglatin[SEP]ar-thay aris-ay orand-ay and-ay andes-ay of[SEP]e-thay e-thay o-thay e-thay and-ay e-thay at-ay and-ay a'\n","iter_dt 53.31ms; iter 800: train loss 1.60858\n","b'translate this to piglatin[SEP]art-thay it-thay e-thay ert-thay enourtiong[SEP]ating-ay ent-ay e-thay intir-ay arce-thay acch-ay and-ay'\n","iter_dt 53.01ms; iter 900: train loss 1.44486\n","b'translate this to piglatin[SEP]ass-tay and-ay o-tay at-tay o-tay on-ay and[SEP]antantint-ay e-thay at-thay in-ay and-ay acitition-ay a-'\n","iter_dt 53.42ms; iter 1000: train loss 1.39565\n","b'translate this to piglatin[SEP]assansast-tay atintin-cay atin-ay in-ay in[SEP]at-thay e-that-tay'\n","iter_dt 54.27ms; iter 1100: train loss 1.24918\n","b'translate this to piglatin[SEP]antlat-tay ition-ay ition-ay igin-ay on-ay in-ay at-thay ighting-bay ition-ay itag-ay olai-ay a-ay i'\n","iter_dt 55.46ms; iter 1200: train loss 1.13432\n","b'translate this to piglatin[SEP]anslatl-tray o-tay ollatin-pay iginan-ay ould-say at-tay iglatin-pay igat-tay at-ay'\n","iter_dt 52.89ms; iter 1300: train loss 0.99268\n","b'translate this to piglatin[SEP]anllslslate-thay a-thay o-tay ignin-pay i-thay'\n","iter_dt 54.31ms; iter 1400: train loss 0.93454\n","b'translate this to piglatin[SEP]anslanse-tray o-thay o-tay iglatin-pay'\n","iter_dt 55.72ms; iter 1500: train loss 0.88864\n","b'translate this to piglatin[SEP]anslanslslans-thay o-tay iglatin-pay in-ay igrn-pay'\n","iter_dt 53.83ms; iter 1600: train loss 0.83373\n","b'translate this to piglatin[SEP]andle-tray is-thay o-tay iglating-pay ightin-pay igting-tay iglatingl-pay ignin-ay'\n","iter_dt 60.53ms; iter 1700: train loss 0.82436\n","b'translate this to piglatin[SEP]anslans-tray o-thay igatin-tay'\n","iter_dt 55.11ms; iter 1800: train loss 0.81587\n","b'translate this to piglatin[SEP]ans-tray is-thay is-tay o-tay, iglatinin-pay'\n","iter_dt 52.64ms; iter 1900: train loss 0.83038\n","b'translate this to piglatin[SEP]anslate-tray is-t-thay o-tay ignin-pay at-o-ppay'\n","iter_dt 54.00ms; iter 2000: train loss 0.79767\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay ighbing-pay ignin-pay igan-pay'\n","iter_dt 53.50ms; iter 2100: train loss 0.76896\n","b'translate this to piglatin[SEP]anslans-tray a-thay is-thay o-tay iglatin-play iglatin-play'\n","iter_dt 56.58ms; iter 2200: train loss 0.72208\n","b'translate this to piglatin[SEP]anste-thay ate-thay o-thay iglatin-tay ignin-pay iglanin-pay'\n","iter_dt 58.03ms; iter 2300: train loss 0.72199\n","b'translate this to piglatin[SEP]anslate-thay is-thay is-thay iglate-pay ignin-pay ignatintin-pay'\n","iter_dt 55.06ms; iter 2400: train loss 0.75509\n","b'translate this to piglatin[SEP]anslate-tray is-thay ig-tay iglatin-pay'\n","iter_dt 57.86ms; iter 2500: train loss 0.64822\n","b'translate this to piglatin[SEP]anslate-tray is-thay is-tay iglatin-pay'\n","iter_dt 56.37ms; iter 2600: train loss 0.72849\n","b'translate this to piglatin[SEP]ate-tray aslate-thay o-tay iglatin-pay'\n","iter_dt 56.57ms; iter 2700: train loss 0.70927\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 54.53ms; iter 2800: train loss 0.65529\n","b'translate this to piglatin[SEP]anslate-tray is-thay iglatin-tay igla-pay'\n","iter_dt 57.54ms; iter 2900: train loss 0.68814\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-thay iglatinn-pay'\n","iter_dt 53.61ms; iter 3000: train loss 0.64725\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 55.70ms; iter 3100: train loss 0.61562\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay ign-pay atin-pay'\n","iter_dt 53.57ms; iter 3200: train loss 0.67391\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 55.86ms; iter 3300: train loss 0.65779\n","b'translate this to piglatin[SEP]atrate-tray ate-thay o-thay iglatin-pay'\n","iter_dt 53.27ms; iter 3400: train loss 0.60397\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 53.20ms; iter 3500: train loss 0.62703\n","b'translate this to piglatin[SEP]anslate-tlay ans-thay iglatinin-pay'\n","iter_dt 55.05ms; iter 3600: train loss 0.62831\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 54.63ms; iter 3700: train loss 0.61391\n","b'translate this to piglatin[SEP]anslate-tray o-thay iglatini-pay iglatin-pay'\n","iter_dt 54.48ms; iter 3800: train loss 0.59506\n","b'translate this to piglatin[SEP]ate-tray is-tray o-tay iglatin-pay'\n","iter_dt 53.71ms; iter 3900: train loss 0.61690\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 54.51ms; iter 4000: train loss 0.57841\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-t-pay iglatin-pay'\n","iter_dt 55.55ms; iter 4100: train loss 0.56024\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 56.81ms; iter 4200: train loss 0.54617\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 50.35ms; iter 4300: train loss 0.57564\n","b'translate this to piglatin[SEP]anslate-tray is-thay i-tay iglatinin-pay'\n","iter_dt 55.29ms; iter 4400: train loss 0.57223\n","b'translate this to piglatin[SEP]anslate-tray is-thay is-tay iglatin-pay'\n","iter_dt 52.57ms; iter 4500: train loss 0.53735\n","b'translate this to piglatin[SEP]anslate-tray is-thay is-tay o-tay iglatin-pay'\n","iter_dt 58.87ms; iter 4600: train loss 0.55303\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 54.59ms; iter 4700: train loss 0.57423\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 54.49ms; iter 4800: train loss 0.54160\n","b'translate this to piglatin[SEP]anslate-tray is-thay i-tay iglatin-pay'\n","iter_dt 55.42ms; iter 4900: train loss 0.51407\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay ightatin-pay'\n","iter_dt 61.93ms; iter 5000: train loss 0.53824\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-thay iglatinin-pay,'\n","iter_dt 59.66ms; iter 5100: train loss 0.52281\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 57.93ms; iter 5200: train loss 0.53118\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay o-pay iglatin-pay'\n","iter_dt 59.30ms; iter 5300: train loss 0.55103\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 57.34ms; iter 5400: train loss 0.59346\n","b'translate this to piglatin[SEP]anslate-tray islate-thay o-tay iglatinting-pay'\n","iter_dt 54.39ms; iter 5500: train loss 0.51647\n","b'translate this to piglatin[SEP]anslate-tray is-thay i-tay iglatinn-pay'\n","iter_dt 59.11ms; iter 5600: train loss 0.54525\n","b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n","iter_dt 56.55ms; iter 5700: train loss 0.50289\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 56.96ms; iter 5800: train loss 0.48327\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 60.54ms; iter 5900: train loss 0.52973\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 57.92ms; iter 6000: train loss 0.53542\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-ay iglatin-tay'\n","iter_dt 56.59ms; iter 6100: train loss 0.48366\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay o-pay iglatin-pay'\n","iter_dt 54.20ms; iter 6200: train loss 0.52027\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n","iter_dt 57.22ms; iter 6300: train loss 0.50713\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 55.71ms; iter 6400: train loss 0.55582\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinatin-pay'\n","iter_dt 57.09ms; iter 6500: train loss 0.49337\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinatin-pay'\n","iter_dt 54.42ms; iter 6600: train loss 0.47640\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 60.15ms; iter 6700: train loss 0.49987\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 59.99ms; iter 6800: train loss 0.50730\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 57.79ms; iter 6900: train loss 0.50109\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay o-tay iglatin-pay'\n","iter_dt 57.29ms; iter 7000: train loss 0.51341\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 56.29ms; iter 7100: train loss 0.52454\n","b'translate this to piglatin[SEP]anslate-tray is-thay ig-tay o-pay iglatin-pay'\n","iter_dt 57.97ms; iter 7200: train loss 0.49634\n","b'translate this to piglatin[SEP]anslate-tray is-thay is-thay o-tay iglatin-pay'\n","iter_dt 58.39ms; iter 7300: train loss 0.53585\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n","iter_dt 52.73ms; iter 7400: train loss 0.55017\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 59.12ms; iter 7500: train loss 0.49529\n","b'translate this to piglatin[SEP]aslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 56.11ms; iter 7600: train loss 0.49161\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinnin-pay'\n","iter_dt 61.61ms; iter 7700: train loss 0.48542\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 58.24ms; iter 7800: train loss 0.51161\n","b'translate this to piglatin[SEP]anslate-tray islate-thay o-tay iglatin-pay'\n","iter_dt 57.55ms; iter 7900: train loss 0.48339\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 58.52ms; iter 8000: train loss 0.50390\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinni-pay'\n","iter_dt 59.48ms; iter 8100: train loss 0.49477\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 56.76ms; iter 8200: train loss 0.46071\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay e-pay iglatin-pay'\n","iter_dt 54.69ms; iter 8300: train loss 0.46850\n","b'translate this to piglatin[SEP]anslate-tray is-tay o-tay iglatin-pay'\n","iter_dt 54.70ms; iter 8400: train loss 0.48641\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglating-pay'\n","iter_dt 57.09ms; iter 8500: train loss 0.46716\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 54.11ms; iter 8600: train loss 0.49213\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 65.08ms; iter 8700: train loss 0.46571\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinin-pay'\n","iter_dt 56.81ms; iter 8800: train loss 0.43186\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 56.91ms; iter 8900: train loss 0.49682\n","b'translate this to piglatin[SEP]anslate-tray is-thay iglatin-pay'\n","iter_dt 62.17ms; iter 9000: train loss 0.48804\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatinatin-pay'\n","iter_dt 57.44ms; iter 9100: train loss 0.45988\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 60.04ms; iter 9200: train loss 0.46490\n","b'translate this to piglatin[SEP]anslate-thay is-thay o-tay iglatinin-pay'\n","iter_dt 57.41ms; iter 9300: train loss 0.46466\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 67.27ms; iter 9400: train loss 0.45370\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 57.56ms; iter 9500: train loss 0.46426\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatin-pay'\n","iter_dt 59.27ms; iter 9600: train loss 0.48823\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-ay iglatin-pay'\n","iter_dt 58.11ms; iter 9700: train loss 0.46365\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-tay iglatining-pay'\n","iter_dt 59.76ms; iter 9800: train loss 0.49505\n","b'translate this to piglatin[SEP]atranslate-tray is-thay o-pay iglatining-pay'\n","iter_dt 59.63ms; iter 9900: train loss 0.44001\n","b'translate this to piglatin[SEP]anslate-tray is-thay o-pay iglatin-ay'\n"]},{"output_type":"execute_result","data":{"text/plain":["Decoder(\n","  (transformer): ModuleDict(\n","    (embedding): Embedding(\n","      (vocab_embeddings): Embedding(258, 128)\n","      (position_embeddings): Embedding(200, 128)\n","    )\n","    (h): ModuleList(\n","      (0-5): 6 x TransformerBlock(\n","        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (attn): GenericSelfAttention(\n","          (k): Linear(in_features=128, out_features=128, bias=True)\n","          (v): Linear(in_features=128, out_features=128, bias=True)\n","          (q): Linear(in_features=128, out_features=128, bias=True)\n","          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (hidden_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (mlp): ModuleDict(\n","          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n","          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n","          (act): NewGELU()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=128, out_features=258, bias=False)\n",")"]},"metadata":{},"execution_count":107}],"source":["model, trainer = train(data, model_type=\"Decoder\",\n","          learning_rate = 5e-4,\n","          batch_size = 32,\n","          max_iters = 10000,\n","          dec_n_layer=6,\n","          dec_n_embd=128,\n","          dec_n_head =4,\n","          enc_n_layer=6,\n","          enc_n_embd=128,\n","          enc_n_head=4)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LA77eMgfCsgA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710208485792,"user_tz":240,"elapsed":203656,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"}},"outputId":"5ef179b1-4877-440e-b707-bea609af79e3"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [03:23<00:00,  2.04s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Exact Match: 93/100 = 93.00% correct\n","BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 13 ref_len = 13)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from sacrebleu.metrics import BLEU\n","\n","def eval(trainer, data, tokenizer):\n","    bleu = BLEU()\n","    results = []\n","    mistakes_printed_already = 0\n","    tgts = []\n","    cands = []\n","    for sent in tqdm(data[10000:10100]):\n","        inp = torch.tensor([tokenizer.encode(sent.split(\"[SEP]\")[0] + \"[SEP]\")])\n","        tgt = bytes(sent.split(\"[SEP]\")[1].split(\"[END]\")[0], \"utf-8\")\n","        cat = generate(model, inp, model.block_size-len(inp[0]), 0.1)\n","        tgt_candidate = tokenizer.decode(cat.cpu().numpy()[0])\n","        tgt_candidate = tgt_candidate.split(b\"[END]\")[0].split(b\"[SEP]\")[1]\n","        # compare the predicted sequence to the true sequence\n","        tgts.append([str(tgt)])\n","        cands.append(str(tgt_candidate))\n","        correct = (tgt == tgt_candidate)\n","        results.append(correct)\n","    results = torch.tensor(results).type(torch.float)\n","    print(\"\\n\\nExact Match: %d/%d = %.2f%% correct\" % (torch.sum(results), len(results), 100*torch.mean(results)))\n","    score = bleu.corpus_score(cands, tgts)\n","    print(score)\n","\n","    return results\n","\n","with torch.no_grad():\n","  results = eval(trainer, data, Tokenizer())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"1lWwrT4r3HGF","outputId":"f6d8c941-9cb1-4f05-f365-233345a7fd4b","executionInfo":{"status":"ok","timestamp":1710210442697,"user_tz":240,"elapsed":352,"user":{"displayName":"Jessica Hernandez","userId":"06284171727800282285"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_d18b077d-738e-46d0-ad17-b2ed352ccf1a\", \"my_llm_implementation.py\", 18396)"]},"metadata":{}}],"source":["#@title Assignment Export - Upload the `my_llm_implementation.py` file to GradeScope\n","\n","with open(\"./my_llm_implementation.py\", \"w\") as f:\n","  f.write(setup_block.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(embedding_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(mha_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(block_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(transformer_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(encoder_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(decoder_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(generate_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(encdec_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","  f.write(pref_generate_def.split(\"#Do not change, it will break the AutoGrader\")[0])\n","\n","# If you decide to do this assignment not on Colab, you'll need to simply find the file\n","from google.colab import files\n","files.download('./my_llm_implementation.py')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}